{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Feeling Fine \n",
    "\n",
    "## Data pre-processing \n",
    "We first need to define some functions to pre process the data. This involves converting the audio file into something the computer can actually understand (numerical values). We are going to use the librosa library and it has some predefined extraction functions. \n",
    "\n",
    "We can extract the following information\n",
    "<img src=\"https://i.ibb.co/sbmCxfK/Screenshot-2020-11-30-at-13-54-26.png\" alt=\"Table of function\" width=\"600\"/>\n",
    "\n",
    "So, according to my research:\n",
    "* Chroma : relates to the 12 different pitches, we will be focused with the short term fourier transformation of the sound files. <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/25/ChromaFeatureCmajorScaleScoreAudioColor.png\" alt=\"(Image of a the 12 different pitches)\" width=\"300\"/>\n",
    "* Melspectogram : This relates to different Mel scale and Spectrogram (Check notebook on more info)\n",
    "    * Mel scale : The mel scale is the result of non-linear transformations on frequencies to make it easier to plot and record the distance between frequencies\n",
    "    * Spectrograms : This is the way we plot audio, y axis is hertz, x axis is time, and there is a color spectrum, which ussually represents the decibles. \n",
    "* Mel Frequency Cepstral Co-efficients (MFCC) : A feature of sound (similar to edges in photos) / the log of the magnitude of the fourier transformation of sound waves ... \n",
    "* Spectral Centroid : The center of mass of the spectrum (also considered the brightness of the sound),\n",
    "* Spectral Bandwidth : the difference between the max and the min of the spectrum (max change in frequency),\n",
    "* Spectral Contrast : The differences between the peaks and the valleys in a spectrum, multiple andwidths calculated,\n",
    "* Roll-Off Frequency : The freqency at which the filter begins to cut off (not sure either)\n",
    "\n",
    "\n",
    "Okay, now we've gone into what we can extract from the sound waves in a bit more detail I'll briefly explain the thought process behind the selection I will make. I'm deciding to use Chroma since it measures the pitch. I'll use MFCC because it's a feature of sound that the model will be able to use well, I'll also include the spectral centroid, spectral Bandwidth, spectral contrast to try and mimic the variation in frequency based on the idea people have more voice cracks depending on their emotions (although though i am aware this might cause some over fitting in the model). I'll also include the melspectogram and finally I will also include the roll-off frequency as well under the assumption that even if I start the sentence with a lot of energy my emotions determine how fast i speak, the speed of my language determines my frequency (talking slower ussually gives out a lower sound), and the roll-off frequency might help determine this (once again might be over fitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa                                             # Audio analyser  \n",
    "import soundfile                                           # Read the audio files\n",
    "import os, glob                                            # Deal with files  \n",
    "import numpy as np                                         # Numpy used to manipulate dataframes\n",
    "from sklearn.model_selection import train_test_split       # For testing and training the model \n",
    "from sklearn.neural_network import MLPClassifier           # The ANN model  \n",
    "from sklearn.metrics import accuracy_score                 # used to test the accuracy of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "The function we are going to define takes in a file name, and flags (which parameters to include in extraction), and then returns a data structure which contains the mean of the extracted information. \n",
    "\n",
    "Flag names :\n",
    "* chroma - Chroma Short Term Fourier Transformation (Pitch)\n",
    "* mfcc - Mel Frequency Cepstral Co-Efficients\n",
    "* mel - Melspectrogram\n",
    "* spec_centroid - Spectral Centroid \n",
    "* spec_bandwidth - Spectral Bandwidth \n",
    "* spec_contrast - Spectral Contrast \n",
    "* roll_off - Roll-Off Frequency \n",
    "\n",
    "This function goes though each flag and then returns the mean value of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extracting features \n",
    "Params : file_name (str), chroma (bool), mfcc (bool), mel (bool), spec_centroid (bool), spec_bandwidth (bool)\n",
    "           spec_contrast (bool), roll_off (bool) \n",
    "'''\n",
    "def extract_feature(file_name, chroma, mfcc, mel, spec_centroid, spec_bandwidth, spec_contrast, roll_off):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        raw_audio = sound_file.read(dtype=\"float32\") \n",
    "        sample_rate = sound_file.samplerate         \n",
    "        extracted_features = np.array([])\n",
    "        stft = np.abs(librosa.stft(raw_audio))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            extracted_features = np.hstack((extracted_features, chroma))\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=raw_audio, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            extracted_features = np.hstack((extracted_features, mfccs))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(raw_audio, sr=sample_rate).T,axis=0)\n",
    "            extracted_features = np.hstack((extracted_features, mel))\n",
    "        if spec_centroid:\n",
    "            spec_centroid = np.mean(librosa.feature.spectral_centroid(y=raw_audio, sr=sample_rate).T,axis=0)\n",
    "            extracted_features = np.hstack((extracted_features, spec_centroid))\n",
    "        if spec_bandwidth:\n",
    "            spec_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=raw_audio, sr=sample_rate).T,axis=0)\n",
    "            extracted_features = np.hstack((extracted_features, spec_bandwidth))\n",
    "        if spec_contrast:\n",
    "            spec_contrast = np.mean(librosa.feature.spectral_contrast(y=raw_audio, sr=sample_rate).T,axis=0)\n",
    "            extracted_features = np.hstack((extracted_features, spec_contrast))\n",
    "        if roll_off:\n",
    "            roll_off = np.mean(librosa.feature.spectral_rolloff(y=raw_audio, sr=sample_rate).T,axis=0)\n",
    "            extracted_features = np.hstack((extracted_features, roll_off))\n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Set\n",
    "\n",
    "In this section we will load up the data set and split it into the training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary off all emotions we can measure\n",
    "emotions = {\n",
    "  '01':'neutral',    # file name XX-XX-01 = neutral \n",
    "  '02':'calm',       # file name XX-XX-02 = calm\n",
    "  '03':'happy',      # file name XX-XX-03 = happy\n",
    "  '04':'sad',        # file name XX-XX-04 = sad\n",
    "  '05':'angry',      # file name XX-XX-05 = angry\n",
    "  '06':'fearful',    # file name XX-XX-06 = fearful\n",
    "  '07':'disgust',    # file name XX-XX-07 = disgust\n",
    "  '08':'surprised'   # file name XX-XX-08 = surprised\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have a dictionary mapping a casted number to an emotion, when going through the data set we are going to load in each entry and then extract it's features. We are then going to split this into training data and test data. We are going to add a parameter for the percentage of data to be in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files, extract the features, and split it into the training and test set\n",
    "def load_data(test_size=0.25):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"../data/Actor_*/*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        feature=extract_feature(file,  chroma=True, mfcc=True, mel=True, spec_centroid=True, spec_bandwidth=True, spec_contrast=True, roll_off=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'librosa.feature' has no attribute 'rolloff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9b1687e40332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-209874c91442>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(test_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0memotion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memotions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mchroma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_centroid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_bandwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_contrast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll_off\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b64efd784493>\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(file_name, chroma, mfcc, mel, spec_centroid, spec_bandwidth, spec_contrast, roll_off)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mextracted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_contrast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroll_off\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mroll_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolloff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mextracted_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll_off\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mextracted_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'librosa.feature' has no attribute 'rolloff'"
     ]
    }
   ],
   "source": [
    "# Get the training and testing data\n",
    "x_train,x_test,y_train,y_test=load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
